version: "3"
services:
  mlflow:
    #build:
    #  context: .
    #  dockerfile: mlflow.Dockerfile
    image: bkt92/mlflow:latest
    restart: on-failure
    command:
      - bash
      - -c
      - "mlflow server --backend-store-uri sqlite:////mlflow/mlflow.db --serve-artifacts --host 0.0.0.0 --port 5000"
    ports:
      - "5000:5000"
    volumes:
      - ./.run_env:/mlflow

  redis:
    image: redis:7.0.11-alpine
    restart: on-failure
    ports:
      - "6379:6379"
    volumes:
      - ./.run_env/cache:/data

  memcached:
    image: memcached:1.6.21
    restart: on-failure
    ports:
      - "11211:11211"
    command:
      - '--memory-limit=1024'

  model_predictor:
    #build:
    #  context: .
    #  dockerfile: api.Dockerfile
    image: bkt92/model_predictor:latest
    restart: on-failure
    depends_on:
      - mlflow
      - redis
      - memcached
    #volumes:
      #- ./docker_run_env/data/raw_data:/model_predictor/data/raw_data
      #- ./docker_run_env/data/train_data:/model_predictor/data/train_data
      #- ./data/model_config:/model_predictor/data/model_config
      #- ./docker_run_env/data/captured_data:/model_predictor/data/captured_data
    ports:
      - 8000:8000
    environment:
      #MODEL_CONFIG_PATH: ${MODEL_CONFIG_PATH}
      MLFLOW_TRACKING_URI: http://host.docker.internal:5000
      REDIS_ENDPOINT: host.docker.internal
      MEMCACHED_ENDPOINT: host.docker.internal
    extra_hosts:
      - "host.docker.internal:host-gateway"
