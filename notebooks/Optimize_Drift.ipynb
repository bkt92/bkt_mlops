{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/scipy/scipy/blob/3c89445b6439f3ce7bffc4cf11c6407c39faedc5/scipy/special/cephes/kolmogorov.c#L714\n",
    "\n",
    "_smirnov(int n, double x)\n",
    "{\n",
    "    double nx, alpha;\n",
    "    double2 AjSum = DD_C_ZERO;\n",
    "    double2 dAjSum = DD_C_ZERO;\n",
    "    double cdf, sf, pdf;\n",
    "\n",
    "    int bUseUpperSum;\n",
    "    int nxfl, n1mxfl, n1mxceil;\n",
    "    ThreeProbs ret;\n",
    "\n",
    "    if (!(n > 0 && x >= 0.0 && x <= 1.0)) {\n",
    "        RETURN_3PROBS(NAN, NAN, NAN);\n",
    "    }\n",
    "    if (n == 1) {\n",
    "        RETURN_3PROBS(1-x, x, 1.0);\n",
    "    }\n",
    "    if (x == 0.0) {\n",
    "        RETURN_3PROBS(1.0, 0.0, 1.0);\n",
    "    }\n",
    "    if (x == 1.0) {\n",
    "        RETURN_3PROBS(0.0, 1.0, 0.0);\n",
    "    }\n",
    "\n",
    "    alpha = modNX(n, x, &nxfl, &nx);\n",
    "    n1mxfl = n - nxfl - (alpha == 0 ? 0 : 1);\n",
    "    n1mxceil = n - nxfl;\n",
    "    /*\n",
    "     * If alpha is 0, don't actually want to include the last term\n",
    "     * in either the lower or upper summations.\n",
    "     */\n",
    "    if (alpha == 0) {\n",
    "        n1mxfl -= 1;\n",
    "        n1mxceil += 1;\n",
    "    }\n",
    "\n",
    "    /* Special case:  x <= 1/n  */\n",
    "    if (nxfl == 0 || (nxfl == 1 && alpha == 0)) {\n",
    "        double t = pow2(1, x, n-1);\n",
    "        pdf = (nx + 1) * t / (1+x);\n",
    "        cdf = x * t;\n",
    "        sf = 1 - cdf;\n",
    "        /* Adjust if x=1/n *exactly* */\n",
    "        if (nxfl == 1) {\n",
    "            assert(alpha == 0);\n",
    "            pdf -= 0.5;\n",
    "        }\n",
    "        RETURN_3PROBS(sf, cdf, pdf);\n",
    "    }\n",
    "    /* Special case:  x is so big, the sf underflows double64 */\n",
    "    if (-2 * n * x*x < MINLOG) {\n",
    "        RETURN_3PROBS(0, 1, 0);\n",
    "    }\n",
    "    /* Special case:  x >= 1 - 1/n */\n",
    "    if (nxfl >= n-1) {\n",
    "        sf = pow2(1, -x, n);\n",
    "        cdf = 1 - sf;\n",
    "        pdf = n * sf/(1-x);\n",
    "        RETURN_3PROBS(sf, cdf, pdf);\n",
    "    }\n",
    "    /* Special case:  n is so big, take too long to compute */\n",
    "    if (n > SMIRNOV_MAX_COMPUTE_N) {\n",
    "        /* p ~ e^(-(6nx+1)^2 / 18n) */\n",
    "        double logp = -pow(6.0*n*x+1, 2)/18.0/n;\n",
    "        /* Maximise precision for small p-value. */\n",
    "        if (logp < -M_LN2) {\n",
    "            sf = exp(logp);\n",
    "            cdf = 1 - sf;\n",
    "        } else {\n",
    "            cdf = -expm1(logp);\n",
    "            sf = 1 - cdf;\n",
    "        }\n",
    "        pdf = (6.0*n*x+1) * 2 * sf/3;\n",
    "        RETURN_3PROBS(sf, cdf, pdf);\n",
    "    }\n",
    "    {\n",
    "        /*\n",
    "         * Use the upper sum if n is large enough, and x is small enough and\n",
    "         * the number of terms is going to be small enough.\n",
    "         * Otherwise it just drops accuracy, about 1.6bits * nUpperTerms\n",
    "         */\n",
    "        int nUpperTerms = n - n1mxceil + 1;\n",
    "        bUseUpperSum = (nUpperTerms <= 1 && x < 0.5);\n",
    "        bUseUpperSum = (bUseUpperSum ||\n",
    "                        ((n >= SM_UPPERSUM_MIN_N)\n",
    "                          && (nUpperTerms <= SM_UPPER_MAX_TERMS)\n",
    "                          && (x <= 0.5 / sqrt(n))));\n",
    "    }\n",
    "\n",
    "    {\n",
    "        int start=0, step=1, nTerms=n1mxfl+1;\n",
    "        int j, firstJ = 0;\n",
    "        int vmid = n/2;\n",
    "        double2 Cman = DD_C_ONE;\n",
    "        int Cexpt = 0;\n",
    "        double2 Aj, dAj, t1, t2, dAjCoeff;\n",
    "        double2 oneOverX = div_dd(1, x);\n",
    "\n",
    "        if (bUseUpperSum) {\n",
    "            start = n;\n",
    "            step = -1;\n",
    "            nTerms = n - n1mxceil + 1;\n",
    "\n",
    "            t1 = pow4_D(1, x, 1, 0, n - 1);\n",
    "            t2 = DD_C_ONE;\n",
    "            Aj = t1;\n",
    "\n",
    "            dAjCoeff = div_dD(n - 1, add_dd(1, x));\n",
    "            dAjCoeff = add_DD(dAjCoeff, oneOverX);\n",
    "        } else {\n",
    "            t1 = oneOverX;\n",
    "            t2 = pow4_D(1, -x, 1, 0, n);\n",
    "            Aj = div_Dd(t2, x);\n",
    "\n",
    "            dAjCoeff = div_DD(sub_dD(-1, mul_dd(n - 1, x)), sub_dd(1, x));\n",
    "            dAjCoeff = div_Dd(dAjCoeff, x);\n",
    "            dAjCoeff = add_DD(dAjCoeff, oneOverX);\n",
    "        }\n",
    "\n",
    "        dAj = mul_DD(Aj, dAjCoeff);\n",
    "        AjSum = add_DD(AjSum, Aj);\n",
    "        dAjSum = add_DD(dAjSum, dAj);\n",
    "\n",
    "        updateBinomial(&Cman, &Cexpt, n, 0);\n",
    "        firstJ ++;\n",
    "\n",
    "        for (j = firstJ; j < nTerms; j += 1) {\n",
    "            int v = start + j * step;\n",
    "\n",
    "            computeAv(n, x, v, Cman, Cexpt, &t1, &t2, &Aj);\n",
    "\n",
    "            if (dd_isfinite(Aj) && !dd_is_zero(Aj)) {\n",
    "                /* coeff = 1/x + (j-1)/(x+j/n) - (n-j)/(1-x-j/n) */\n",
    "                dAjCoeff = sub_DD(div_dD((n * (v - 1)), add_dd(nxfl + v, alpha)),\n",
    "                            div_dD(((n - v) * n), sub_dd(n - nxfl - v, alpha)));\n",
    "                dAjCoeff = add_DD(dAjCoeff, oneOverX);\n",
    "                dAj = mul_DD(Aj, dAjCoeff);\n",
    "\n",
    "                assert(dd_isfinite(Aj));\n",
    "                AjSum = add_DD(AjSum, Aj);\n",
    "                dAjSum = add_DD(dAjSum, dAj);\n",
    "            }\n",
    "            /* Safe to terminate early? */\n",
    "            if (!dd_is_zero(Aj)) {\n",
    "               if ((4*(nTerms-j) * fabs(dd_to_double(Aj)) < DBL_EPSILON * dd_to_double(AjSum))\n",
    "                     && (j != nTerms - 1)) {\n",
    "                   break;\n",
    "                }\n",
    "            }\n",
    "            else if (j > vmid) {\n",
    "                assert(dd_is_zero(Aj));\n",
    "                break;\n",
    "            }\n",
    "\n",
    "            updateBinomial(&Cman, &Cexpt, n, j);\n",
    "        }\n",
    "        assert(dd_isfinite(AjSum));\n",
    "        assert(dd_isfinite(dAjSum));\n",
    "        {\n",
    "            double2 derivD = mul_dD(x, dAjSum);\n",
    "            double2 probD = mul_dD(x, AjSum);\n",
    "            double deriv = dd_to_double(derivD);\n",
    "            double prob = dd_to_double(probD);\n",
    "\n",
    "            assert (nx != 1 || alpha > 0);\n",
    "            if (step < 0) {\n",
    "                cdf = prob;\n",
    "                sf = 1-prob;\n",
    "                pdf = deriv;\n",
    "            } else {\n",
    "                cdf = 1-prob;\n",
    "                sf = prob;\n",
    "                pdf = -deriv;\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    pdf = MAX(0, pdf);\n",
    "    cdf = CLIP(cdf, 0, 1);\n",
    "    sf = CLIP(sf, 0, 1);\n",
    "    RETURN_3PROBS(sf, cdf, pdf);\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Todo: optimize the smirnov function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import smirnov\n",
    "from numba import njit, int32, float64, boolean\n",
    "\n",
    "_E128 = 128\n",
    "_EP128 = np.ldexp(np.double(1), _E128)\n",
    "_EM128 = np.ldexp(np.double(1), -_E128)\n",
    "_SQRT2PI = np.sqrt(2 * np.pi)\n",
    "_LOG_2PI = np.log(2 * np.pi)\n",
    "_PI_SQUARED = np.pi ** 2\n",
    "_PI_FOUR = np.pi ** 4\n",
    "_PI_SIX = np.pi ** 6\n",
    "_SQRT3 = np.sqrt(3)\n",
    "_MIN_LOG = -708\n",
    "_STIRLING_COEFFS = np.array([-2.955065359477124183e-2, 6.4102564102564102564e-3,\n",
    "                    -1.9175269175269175269e-3, 8.4175084175084175084e-4,\n",
    "                    -5.952380952380952381e-4, 7.9365079365079365079e-4,\n",
    "                    -2.7777777777777777778e-3, 8.3333333333333333333e-2])\n",
    "\n",
    "@njit()\n",
    "def polyval(p: list, x):\n",
    "    y = 0\n",
    "    for i in range(len(p)):\n",
    "        y = x * y + p[i]\n",
    "    return y\n",
    "\n",
    "@njit()\n",
    "def _clip_prob(p):\n",
    "    \"\"\"clips a probability to range 0<=p<=1.\"\"\"\n",
    "    #return np.clip(p, 0.0, 1.0)\n",
    "    return min(max(p, 0.0), 1.0)\n",
    "\n",
    "@njit()\n",
    "def _select_and_clip_prob(cdfprob, sfprob, cdf=True):\n",
    "    \"\"\"Selects either the CDF or SF, and then clips to range 0<=p<=1.\"\"\"\n",
    "    #p = np.where(cdf, cdfprob, sfprob)\n",
    "    p = cdfprob if cdf else sfprob\n",
    "    return _clip_prob(p)\n",
    "\n",
    "@njit(float64(int32))\n",
    "def _log_nfactorial_div_n_pow_n(n):\n",
    "    # Computes n! / n**n\n",
    "    #    = (n-1)! / n**(n-1)\n",
    "    # Uses Stirling's approximation, but removes n*log(n) up-front to\n",
    "    # avoid subtractive cancellation.\n",
    "    #    = log(n)/2 - n + log(sqrt(2pi)) + sum B_{2j}/(2j)/(2j-1)/n**(2j-1)\n",
    "    rn = 1.0/n\n",
    "    return np.log(n)/2 - n + _LOG_2PI/2 + rn * polyval(_STIRLING_COEFFS, rn/n)\n",
    "\n",
    "@njit(float64(int32, float64, boolean))\n",
    "def _kolmogn_DMTW(n: int, d: float, cdf=True):\n",
    "    r\"\"\"Computes the Kolmogorov CDF:  Pr(D_n <= d) using the MTW approach to\n",
    "    the Durbin matrix algorithm.\n",
    "\n",
    "    Durbin (1968); Marsaglia, Tsang, Wang (2003). [1], [3].\n",
    "    \"\"\"\n",
    "    # Write d = (k-h)/n, where k is positive integer and 0 <= h < 1\n",
    "    # Generate initial matrix H of size m*m where m=(2k-1)\n",
    "    # Compute k-th row of (n!/n^n) * H^n, scaling intermediate results.\n",
    "    # Requires memory O(m^2) and computation O(m^2 log(n)).\n",
    "    # Most suitable for small m.\n",
    "\n",
    "    if d >= 1.0:\n",
    "        return _select_and_clip_prob(1.0, 0.0, cdf)\n",
    "    nd = n * d\n",
    "    if nd <= 0.5:\n",
    "        return _select_and_clip_prob(0.0, 1.0, cdf)\n",
    "    k = int(np.ceil(nd))\n",
    "    h = k - nd\n",
    "    m = 2 * k - 1\n",
    "\n",
    "    H = np.zeros((m, m))\n",
    "\n",
    "    # Initialize: v is first column (and last row) of H\n",
    "    #  v[j] = (1-h^(j+1)/(j+1)!  (except for v[-1])\n",
    "    #  w[j] = 1/(j)!\n",
    "    # q = k-th row of H (actually i!/n^i*H^i)\n",
    "    intm = np.arange(1, m + 1)\n",
    "    v = 1.0 - h ** intm\n",
    "    w = np.zeros(m)\n",
    "    fac = 1.0\n",
    "    for j in intm:\n",
    "        w[j - 1] = fac\n",
    "        fac /= j  # This might underflow.  Isn't a problem.\n",
    "        v[j - 1] *= fac\n",
    "    tt = max(2 * h - 1.0, 0)**m - 2*h**m\n",
    "    v[-1] = (1.0 + tt) * fac\n",
    "\n",
    "    for i in range(1, m):\n",
    "        H[i - 1:, i] = w[:m - i + 1]\n",
    "    H[:, 0] = v\n",
    "    H[-1, :] = np.flipud(v)\n",
    "\n",
    "    Hpwr = np.eye(np.shape(H)[0])  # Holds intermediate powers of H\n",
    "    nn = n\n",
    "    expnt = 0  # Scaling of Hpwr\n",
    "    Hexpnt = 0  # Scaling of H\n",
    "    while nn > 0:\n",
    "        if nn % 2:\n",
    "            Hpwr = np.dot(Hpwr, H)\n",
    "            expnt += Hexpnt\n",
    "        H = np.dot(H, H)\n",
    "        Hexpnt *= 2\n",
    "        # Scale as needed.\n",
    "        if np.abs(H[k - 1, k - 1]) > _EP128:\n",
    "            H /= _EP128\n",
    "            Hexpnt += _E128\n",
    "        nn = nn // 2\n",
    "\n",
    "    p = Hpwr[k - 1, k - 1]\n",
    "\n",
    "    # Multiply by n!/n^n\n",
    "    for i in range(1, n + 1):\n",
    "        p = i * p / n\n",
    "        if np.abs(p) < _EM128:\n",
    "            p *= _EP128\n",
    "            expnt -= _E128\n",
    "\n",
    "    # unscale\n",
    "    if expnt != 0:\n",
    "        p = p*np.exp2(expnt)\n",
    "\n",
    "    return _select_and_clip_prob(p, 1.0-p, cdf)\n",
    "\n",
    "@njit()\n",
    "def _pomeranz_compute_j1j2(i, n, ll, ceilf, roundf):\n",
    "    \"\"\"Compute the endpoints of the interval for row i.\"\"\"\n",
    "    if i == 0:\n",
    "        j1, j2 = -ll - ceilf - 1, ll + ceilf - 1\n",
    "    else:\n",
    "        # i + 1 = 2*ip1div2 + ip1mod2\n",
    "        ip1div2, ip1mod2 = divmod(i + 1, 2)\n",
    "        if ip1mod2 == 0:  # i is odd\n",
    "            if ip1div2 == n + 1:\n",
    "                j1, j2 = n - ll - ceilf - 1, n + ll + ceilf - 1\n",
    "            else:\n",
    "                j1, j2 = ip1div2 - 1 - ll - roundf - 1, ip1div2 + ll - 1 + ceilf - 1\n",
    "        else:\n",
    "            j1, j2 = ip1div2 - 1 - ll - 1, ip1div2 + ll + roundf - 1\n",
    "\n",
    "    return max(j1 + 2, 0), min(j2, n)\n",
    "\n",
    "@njit(float64(int32, float64, boolean))\n",
    "def _kolmogn_Pomeranz(n: int, x: float, cdf=True):\n",
    "    r\"\"\"Computes Pr(D_n <= d) using the Pomeranz recursion algorithm.\n",
    "\n",
    "    Pomeranz (1974) [2]\n",
    "    \"\"\"\n",
    "    # V is n*(2n+2) matrix.\n",
    "    # Each row is convolution of the previous row and probabilities from a\n",
    "    #  Poisson distribution.\n",
    "    # Desired CDF probability is n! V[n-1, 2n+1]  (final entry in final row).\n",
    "    # Only two rows are needed at any given stage:\n",
    "    #  - Call them V0 and V1.\n",
    "    #  - Swap each iteration\n",
    "    # Only a few (contiguous) entries in each row can be non-zero.\n",
    "    #  - Keep track of start and end (j1 and j2 below)\n",
    "    #  - V0s and V1s track the start in the two rows\n",
    "    # Scale intermediate results as needed.\n",
    "    # Only a few different Poisson distributions can occur\n",
    "    t = n * x\n",
    "    ll = int(np.floor(t))\n",
    "    f = 1.0 * (t - ll)  # fractional part of t\n",
    "    g = min(f, 1.0 - f)\n",
    "    ceilf = (1 if f > 0 else 0)\n",
    "    roundf = (1 if f > 0.5 else 0)\n",
    "    npwrs = 2 * (ll + 1)    # Maximum number of powers needed in convolutions\n",
    "    gpower = np.empty(npwrs)  # gpower = (g/n)^m/m!\n",
    "    twogpower = np.empty(npwrs)  # twogpower = (2g/n)^m/m!\n",
    "    onem2gpower = np.empty(npwrs)  # onem2gpower = ((1-2g)/n)^m/m!\n",
    "    # gpower etc are *almost* Poisson probs, just missing normalizing factor.\n",
    "\n",
    "    gpower[0] = 1.0\n",
    "    twogpower[0] = 1.0\n",
    "    onem2gpower[0] = 1.0\n",
    "    expnt = 0\n",
    "    g_over_n, two_g_over_n, one_minus_two_g_over_n = g/n, 2*g/n, (1 - 2*g)/n\n",
    "    for m in range(1, npwrs):\n",
    "        gpower[m] = gpower[m - 1] * g_over_n / m\n",
    "        twogpower[m] = twogpower[m - 1] * two_g_over_n / m\n",
    "        onem2gpower[m] = onem2gpower[m - 1] * one_minus_two_g_over_n / m\n",
    "\n",
    "    V0 = np.zeros((npwrs))\n",
    "    V1 = np.zeros((npwrs))\n",
    "    V1[0] = 1  # first row\n",
    "    V0s, V1s = 0, 0  # start indices of the two rows\n",
    "\n",
    "    j1, j2 = _pomeranz_compute_j1j2(0, n, ll, ceilf, roundf)\n",
    "    for i in range(1, 2 * n + 2):\n",
    "        # Preserve j1, V1, V1s, V0s from last iteration\n",
    "        k1 = j1\n",
    "        V0, V1 = V1, V0\n",
    "        V0s, V1s = V1s, V0s\n",
    "        V1.fill(0.0)\n",
    "        j1, j2 = _pomeranz_compute_j1j2(i, n, ll, ceilf, roundf)\n",
    "        if i == 1 or i == 2 * n + 1:\n",
    "            pwrs = gpower\n",
    "        else:\n",
    "            pwrs = (twogpower if i % 2 else onem2gpower)\n",
    "        ln2 = j2 - k1 + 1\n",
    "        if ln2 > 0:\n",
    "            conv = np.convolve(V0[k1 - V0s:k1 - V0s + ln2], pwrs[:ln2])\n",
    "            conv_start = j1 - k1  # First index to use from conv\n",
    "            conv_len = j2 - j1 + 1  # Number of entries to use from conv\n",
    "            V1[:conv_len] = conv[conv_start:conv_start + conv_len]\n",
    "            # Scale to avoid underflow.\n",
    "            if 0 < np.max(V1) < np.double(1)*np.exp2(-128):\n",
    "                V1 *= np.double(1)*np.exp2(128)\n",
    "                expnt -= 128\n",
    "            V1s = V0s + j1 - k1\n",
    "\n",
    "    # multiply by n!\n",
    "    ans = V1[n - V1s]\n",
    "    for m in range(1, n + 1):\n",
    "        if np.abs(ans) > np.double(1)*np.exp2(128):\n",
    "            ans *= np.double(1)*np.exp2(-128)\n",
    "            expnt += 128\n",
    "        ans *= m\n",
    "    ansx: float\n",
    "    # Undo any intermediate scaling\n",
    "    if expnt != 0:\n",
    "        ans = ans*np.exp2(expnt)\n",
    "        \n",
    "    ans = _select_and_clip_prob(ans, 1.0 - ans, cdf)\n",
    "    return ans\n",
    "\n",
    "@njit(float64(int32, float64, boolean))\n",
    "def _kolmogn_PelzGood(n, x, cdf=True):\n",
    "    \"\"\"Computes the Pelz-Good approximation to Prob(Dn <= x) with 0<=x<=1.\n",
    "\n",
    "    Start with Li-Chien, Korolyuk approximation:\n",
    "        Prob(Dn <= x) ~ K0(z) + K1(z)/sqrt(n) + K2(z)/n + K3(z)/n**1.5\n",
    "    where z = x*sqrt(n).\n",
    "    Transform each K_(z) using Jacobi theta functions into a form suitable\n",
    "    for small z.\n",
    "    Pelz-Good (1976). [6]\n",
    "    \"\"\"\n",
    "    if x <= 0.0:\n",
    "        return _select_and_clip_prob(0.0, 1.0, cdf=cdf)\n",
    "    if x >= 1.0:\n",
    "        return _select_and_clip_prob(1.0, 0.0, cdf=cdf)\n",
    "\n",
    "    z = np.sqrt(n) * x\n",
    "    zsquared, zthree, zfour, zsix = z**2, z**3, z**4, z**6\n",
    "\n",
    "    qlog = -_PI_SQUARED / 8 / zsquared\n",
    "    if qlog < _MIN_LOG:  # z ~ 0.041743441416853426\n",
    "        return _select_and_clip_prob(0.0, 1.0, cdf=cdf)\n",
    "\n",
    "    q = np.exp(qlog)\n",
    "\n",
    "    # Coefficients of terms in the sums for K1, K2 and K3\n",
    "    k1a = -zsquared\n",
    "    k1b = _PI_SQUARED / 4\n",
    "\n",
    "    k2a = 6 * zsix + 2 * zfour\n",
    "    k2b = (2 * zfour - 5 * zsquared) * _PI_SQUARED / 4\n",
    "    k2c = _PI_FOUR * (1 - 2 * zsquared) / 16\n",
    "\n",
    "    k3d = _PI_SIX * (5 - 30 * zsquared) / 64\n",
    "    k3c = _PI_FOUR * (-60 * zsquared + 212 * zfour) / 16\n",
    "    k3b = _PI_SQUARED * (135 * zfour - 96 * zsix) / 4\n",
    "    k3a = -30 * zsix - 90 * z**8\n",
    "\n",
    "    K0to3 = np.zeros(4)\n",
    "    # Use a Horner scheme to evaluate sum c_i q^(i^2)\n",
    "    # Reduces to a sum over odd integers.\n",
    "    maxk = int(np.ceil(16 * z / np.pi))\n",
    "    \n",
    "    for k in range(maxk, 0, -1):\n",
    "        m = 2 * k - 1\n",
    "        msquared, mfour, msix = m**2, m**4, m**6\n",
    "        qpower = np.power(q, 8 * k)\n",
    "        coeffs = np.array([1.0,\n",
    "                           k1a + k1b*msquared,\n",
    "                           k2a + k2b*msquared + k2c*mfour,\n",
    "                           k3a + k3b*msquared + k3c*mfour + k3d*msix])\n",
    "        K0to3 *= qpower\n",
    "        K0to3 += coeffs\n",
    "    K0to3 *= q\n",
    "    K0to3 *= _SQRT2PI\n",
    "    # z**10 > 0 as z > 0.04\n",
    "    K0to3 /= np.array([z, 6 * zfour, 72 * z**7, 6480 * z**10])\n",
    "\n",
    "    # Now do the other sum over the other terms, all integers k\n",
    "    # K_2:  (pi^2 k^2) q^(k^2),\n",
    "    # K_3:  (3pi^2 k^2 z^2 - pi^4 k^4)*q^(k^2)\n",
    "    # Don't expect much subtractive cancellation so use direct calculation\n",
    "    q = np.exp(-_PI_SQUARED / 2 / zsquared)\n",
    "    ks = np.arange(maxk, 0, -1)\n",
    "    ksquared = ks ** 2\n",
    "    sqrt3z = _SQRT3 * z\n",
    "    kspi = np.pi * ks\n",
    "    qpwers = q ** ksquared\n",
    "    k2extra = np.sum(ksquared * qpwers)\n",
    "    k2extra *= _PI_SQUARED * _SQRT2PI/(-36 * zthree)\n",
    "    K0to3[2] += k2extra\n",
    "    k3extra = np.sum((sqrt3z + kspi) * (sqrt3z - kspi) * ksquared * qpwers)\n",
    "    k3extra *= _PI_SQUARED * _SQRT2PI/(216 * zsix)\n",
    "    K0to3[3] += k3extra\n",
    "    powers_of_n = np.power(n * 1.0, np.arange(len(K0to3)) / 2.0)\n",
    "    K0to3 /= powers_of_n\n",
    "\n",
    "    if not cdf:\n",
    "        K0to3 *= -1\n",
    "        K0to3[0] += 1\n",
    "\n",
    "    Ksum = np.sum(K0to3)\n",
    "    return Ksum\n",
    "\n",
    "def kolmogn(n, x, cdf=True):\n",
    "    \"\"\"Computes the CDF(or SF) for the two-sided Kolmogorov-Smirnov statistic.\n",
    "\n",
    "    x must be of type float, n of type integer.\n",
    "\n",
    "    Simard & L'Ecuyer (2011) [7].\n",
    "    \"\"\"\n",
    "    if np.isnan(n):\n",
    "        return n  # Keep the same type of nan\n",
    "    if int(n) != n or n <= 0:\n",
    "        return np.nan\n",
    "    if x >= 1.0:\n",
    "        return _select_and_clip_prob(1.0, 0.0, cdf=cdf)\n",
    "    if x <= 0.0:\n",
    "        return _select_and_clip_prob(0.0, 1.0, cdf=cdf)\n",
    "    t = n * x\n",
    "    if t <= 1.0:  # Ruben-Gambino: 1/2n <= x <= 1/n\n",
    "        if t <= 0.5:\n",
    "            return _select_and_clip_prob(0.0, 1.0, cdf=cdf)\n",
    "        if n <= 140:\n",
    "            prob = np.prod(np.arange(1, n+1) * (1.0/n) * (2*t - 1))\n",
    "        else:\n",
    "            prob = np.exp(_log_nfactorial_div_n_pow_n(n) + n * np.log(2*t-1))\n",
    "        return _select_and_clip_prob(prob, 1.0 - prob, cdf=cdf)\n",
    "    if t >= n - 1:  # Ruben-Gambino\n",
    "        prob = 2 * (1.0 - x)**n\n",
    "        return _select_and_clip_prob(1 - prob, prob, cdf=cdf)\n",
    "    if x >= 0.5:  # Exact: 2 * smirnov\n",
    "        prob = 2 * smirnov(n, x)\n",
    "        return _select_and_clip_prob(1.0 - prob, prob, cdf=cdf)\n",
    "\n",
    "    nxsquared = t * x\n",
    "    if n <= 140:\n",
    "        if nxsquared <= 0.754693:\n",
    "            prob = _kolmogn_DMTW(n, x, cdf=True)\n",
    "            return _select_and_clip_prob(prob, 1.0 - prob, cdf=cdf)\n",
    "        if nxsquared <= 4:\n",
    "            prob = _kolmogn_Pomeranz(n, x, cdf=True)\n",
    "            return _select_and_clip_prob(prob, 1.0 - prob, cdf=cdf)\n",
    "        # Now use Miller approximation of 2*smirnov\n",
    "        prob = 2 * smirnov(n, x)\n",
    "        return _select_and_clip_prob(1.0 - prob, prob, cdf=cdf)\n",
    "\n",
    "    # Split CDF and SF as they have different cutoffs on nxsquared.\n",
    "    if not cdf:\n",
    "        if nxsquared >= 370.0:\n",
    "            return 0.0\n",
    "        if nxsquared >= 2.2:\n",
    "            prob = 2 * smirnov(n, x)\n",
    "            return np.clip(prob, 0.0, 1.0)\n",
    "        # Fall through and compute the SF as 1.0-CDF\n",
    "    if nxsquared >= 18.0:\n",
    "        cdfprob = 1.0\n",
    "    elif n <= 100000 and n * x**1.5 <= 1.4:\n",
    "        cdfprob = _kolmogn_DMTW(n, x, cdf=True)\n",
    "    else:\n",
    "        cdfprob = _kolmogn_PelzGood(n, x, cdf=True)\n",
    "    return _select_and_clip_prob(cdfprob, 1.0 - cdfprob, cdf=cdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ks_2samp(data1, data2, alternative='two-sided'):\n",
    "\n",
    "    alternative = {'t': 'two-sided', 'g': 'greater', 'l': 'less'}.get(\n",
    "        alternative.lower()[0], alternative)\n",
    "    if alternative not in ['two-sided', 'less', 'greater']:\n",
    "        raise ValueError(f'Invalid value for alternative: {alternative}')\n",
    "    if np.ma.is_masked(data1):\n",
    "        data1 = data1.compressed()\n",
    "    if np.ma.is_masked(data2):\n",
    "        data2 = data2.compressed()\n",
    "    data1 = np.sort(data1)\n",
    "    data2 = np.sort(data2)\n",
    "    n1 = data1.shape[0]\n",
    "    n2 = data2.shape[0]\n",
    "    if min(n1, n2) == 0:\n",
    "        raise ValueError('Data passed to ks_2samp must not be empty')\n",
    "\n",
    "    data_all = np.concatenate([data1, data2])\n",
    "    # using searchsorted solves equal data problem\n",
    "    cdf1 = np.searchsorted(data1, data_all, side='right') / n1\n",
    "    cdf2 = np.searchsorted(data2, data_all, side='right') / n2\n",
    "    cddiffs = cdf1 - cdf2\n",
    "\n",
    "    # Identify the location of the statistic\n",
    "    argminS = np.argmin(cddiffs)\n",
    "    argmaxS = np.argmax(cddiffs)\n",
    "\n",
    "    # Ensure sign of minS is not negative.\n",
    "    minS = np.clip(-cddiffs[argminS], 0, 1)\n",
    "    maxS = cddiffs[argmaxS]\n",
    "\n",
    "    if alternative == 'less' or (alternative == 'two-sided' and minS > maxS):\n",
    "        d = minS\n",
    "    else:\n",
    "        d = maxS\n",
    "\n",
    "    prob = -np.inf\n",
    "\n",
    "    m, n = sorted([float(n1), float(n2)], reverse=True)\n",
    "    en = m * n / (m + n)\n",
    "    if alternative == 'two-sided':\n",
    "        prob = kolmogn(x=d, n=int(np.round(en)),cdf=False)\n",
    "    else:\n",
    "        z = np.sqrt(en) * d\n",
    "        expt = -2 * z**2 - 2 * z * (m + 2*n)/np.sqrt(m*n*(m+n))/3.0\n",
    "        prob = np.exp(expt)\n",
    "\n",
    "    prob = np.clip(prob, 0, 1)\n",
    "    return d, prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ks_drift_detect(x_ref: np.ndarray, x: np.ndarray, p_val: np.float32 = 0.05):\n",
    "    \"\"\"\n",
    "    Check data drift by using K-S score (code from alibi_detect).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x_ref\n",
    "        Reference instances to compare distribution with.\n",
    "    x\n",
    "        Batch of instances.\n",
    "\n",
    "    p_val\n",
    "        Threshold for detection.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    is drift: 0 no drift or 1 drift.\n",
    "    \"\"\"\n",
    "    x = x.reshape(x.shape[0], -1)\n",
    "    x_ref = x_ref.reshape(x_ref.shape[0], -1)\n",
    "    n_features = x_ref.reshape(x_ref.shape[0], -1).shape[-1]\n",
    "    p_vals = np.zeros(n_features, dtype=np.float32)\n",
    "    dist = np.zeros_like(p_vals)\n",
    "    for f in range(n_features):\n",
    "        dist[f], p_vals[f] = ks_2samp(x_ref[:, f], x[:, f], alternative= 'two-sided')\n",
    "    threshold = p_val / n_features\n",
    "    drift_pred = int((p_vals < threshold).any()) \n",
    "    return drift_pred, p_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9930481 , 0.9999609 , 1.        , 0.99992096, 0.9880359 ,\n",
       "       0.9512366 , 0.9585877 , 0.9343235 , 0.81019986, 0.99992096,\n",
       "       0.8919347 , 0.77935463, 0.999541  , 0.99992096, 0.8394735 ,\n",
       "       0.77935463, 0.81019986, 0.6302808 , 1.        , 0.7309107 ,\n",
       "       0.45293587, 0.9999971 , 0.99973094, 0.9652068 , 0.99973094,\n",
       "       0.5966983 , 0.9973971 , 1.        , 0.99999994, 0.9973971 ,\n",
       "       0.9930481 , 0.5148559 , 0.99999994, 0.98083913, 0.8668274 ,\n",
       "       1.        , 1.        , 1.        , 0.9145085 , 0.879681  ,\n",
       "       1.        ], dtype=float32)"
      ]
     },
     "execution_count": 704,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ks_drift_detect(X_baseline, x)[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.51 ms ± 475 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1\n",
    "ks_drift_detect(X_baseline, x)[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9930481 , 0.9999609 , 1.        , 0.99992096, 0.9880359 ,\n",
       "       0.9512366 , 0.9585877 , 0.9343235 , 0.81019986, 0.99992096,\n",
       "       0.8919347 , 0.77935463, 0.999541  , 0.99992096, 0.8394735 ,\n",
       "       0.77935463, 0.81019986, 0.6302808 , 1.        , 0.7309107 ,\n",
       "       0.45293587, 0.9999971 , 0.99973094, 0.9652068 , 0.99973094,\n",
       "       0.5966983 , 0.9973971 , 1.        , 0.99999994, 0.9973971 ,\n",
       "       0.9930481 , 0.5148559 , 0.99999994, 0.98083913, 0.8668274 ,\n",
       "       1.        , 1.        , 1.        , 0.9145085 , 0.879681  ,\n",
       "       1.        ], dtype=float32)"
      ]
     },
     "execution_count": 694,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cd.predict(x)['data']['p_val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%timeit -n 10\n",
    "for i in range(100):\n",
    "    x=train_x.sample(2000).to_numpy()\n",
    "    assert (ks_drift_detect(X_baseline, x)[1] == cd.predict(x)['data']['p_val']).sum() == 41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alibi_detect.cd import KSDrift\n",
    "cd = KSDrift(p_val=0.05, x_ref=X_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.4 ms ± 498 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 10\n",
    "cd.predict(x)['data']['p_val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\VENV\\api_prediction\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "%pwd\n",
    "import sys\n",
    "sys.path.append('./src')\n",
    "from src.data_processor import RawDataProcessor\n",
    "from src.problem_config import create_prob_config \n",
    "prob_config = create_prob_config(\"phase-2\", \"prob-1\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "training_data = pd.read_parquet(prob_config.raw_data_path)\n",
    "\n",
    "training_data, category_index = RawDataProcessor.build_category_features(\n",
    "            training_data, prob_config.categorical_cols\n",
    "        )\n",
    "\n",
    "target_col = prob_config.target_col\n",
    "train_x = training_data.drop([target_col], axis=1)\n",
    "train_y = training_data[[target_col]]\n",
    "\n",
    "# Store the category_index\n",
    "with open(prob_config.category_index_path, \"wb\") as f:\n",
    "    pickle.dump(category_index, f)\n",
    "\n",
    "\n",
    "X_baseline = train_x.sample(100).to_numpy()\n",
    "x=train_x.sample(1000).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_baseline = train_x.sample(500).to_numpy()\n",
    "x=train_x.sample(1000).to_numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
